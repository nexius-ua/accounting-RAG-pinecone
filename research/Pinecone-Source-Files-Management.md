# Аналіз: Чи потрібно архівувати оригінальні файли після завантаження в Pinecone?

## Коротка відповідь

**НІ, технічно не потрібно.** Pinecone автоматично перезаписує записи з однаковими ID при повторному upsert.

---

## Як працює Pinecone

### Поведінка Upsert

З документації Pinecone:

> "If a record ID already exists, upserting **overwrites the entire record**."

Це означає:
- Якщо завантажити документ повторно з тим самим ID — старий запис буде замінено
- Конфлікту не виникне — просто оновлення даних

### Як генеруються ID у нашому скрипті

```python
# scripts/chunk_and_upload.py:86-89
def generate_id(filename: str, chunk_index: int, text: str) -> str:
    hash_input = f"{filename}_{chunk_index}_{text[:50]}"
    return hashlib.md5(hash_input.encode()).hexdigest()[:16]
```

ID = хеш від (назва файлу + номер чанка + перші 50 символів тексту)

**Наслідки:**
| Сценарій | Результат |
|----------|-----------|
| Той самий файл → повторний upsert | Перезапише існуючі чанки (ОК) |
| Новий файл з іншою назвою | Створить нові записи (ОК) |
| Змінений файл з тією ж назвою | Частково перезапише, частково створить нові |

---

## Потенційні проблеми

### 1. Зміна існуючого файлу

Якщо ви **зміните вміст** файлу (не назву), а потім завантажите:
- Нові чанки матимуть **нові ID** (бо текст змінився)
- **Старі чанки залишаться** в індексі як "сироти"

**Рішення:** Перед повторним завантаженням зміненого файлу — видалити старі записи:

```python
# Видалити всі записи з певного файлу
index.delete(
    namespace="default",
    filter={"filename": {"$eq": "Gem 10 Експертна оцінка.md"}}
)
```

### 2. Відсутність метаданих для фільтрації

Наш скрипт зберігає `filename` як метадані — це дозволяє:
- Фільтрувати пошук по файлах
- Видаляти записи конкретного файлу
- Відстежувати джерело кожного чанка

---

## Рекомендована структура папок

```
project/
├── source_docs/               # Нові документи для завантаження
├── archived_source_docs/      # Завантажені документи (для організації)
├── scripts/
│   └── chunk_and_upload.py
└── ...
```

### Навіщо архівувати (організаційні причини)

1. **Візуальне розмежування** — зрозуміло які файли вже в базі
2. **Запобігання повторним завантаженням** — не витрачати час на дублювання
3. **Історія версій** — якщо потрібно відновити або порівняти
4. **Аудит** — знати що саме було завантажено

---

## Кращі практики

### Варіант А: Використовувати Namespaces

Замість архівування — розділяти дані по namespaces:

```python
index.upsert_records("batch-2025-12", records)  # Нові документи
index.upsert_records("batch-2025-11", records)  # Попередні
```

### Варіант Б: Версіонування в метаданих

```python
records.append({
    "_id": generate_id(...),
    "text": chunk,
    "filename": filepath.name,
    "upload_date": "2025-12-01",
    "version": "1.0",
    ...
})
```

### Варіант В: Tracking файл

Зберігати JSON з інформацією про завантажені файли:

```json
{
  "uploaded_files": [
    {"filename": "Gem 10.md", "chunks": 23, "date": "2025-12-01"},
    {"filename": "Gem 11.md", "chunks": 21, "date": "2025-12-01"}
  ]
}
```

---

## Висновок

| Питання | Відповідь |
|---------|-----------|
| Чи перезапишуться дані при повторному завантаженні? | Так, автоматично |
| Чи виникне конфлікт з новими документами? | Ні, якщо назви різні |
| Чи потрібно архівувати? | Технічно ні, але організаційно корисно |
| Що робити зі зміненими файлами? | Видалити старі записи перед повторним завантаженням |

**Рекомендація:** Архівування — хороша практика для організації, але не технічна необхідність.
